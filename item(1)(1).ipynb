{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "item(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbPHiPbiKHBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('Merged.csv', error_bad_lines=False,sep='\\t')\n",
        "test=pd.read_csv('Test.csv', error_bad_lines=False,sep='\\t')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRyjHeLXfNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_OxAxVqKRZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h346JXx1Kfmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/Img.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(r'/content/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPnPDiMBZazp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AtY11XdPZcZT",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/test.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(r'/content/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxr6TrUtKHC6",
        "colab_type": "code",
        "outputId": "d0fde29f-59c5-4213-dfbf-7f1e56e9beb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyds= df.filter(['Item', 'img'], axis=1)\n",
        "dt= test.filter(['Item', 'img'], axis=1)\n",
        "dt.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item</th>\n",
              "      <th>img</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>79610782-69fe-4653-8c43-9024366a9a581556619128...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>6b60f13a-8d7d-4e0d-abac-ab83b58882bc1556619128...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>3711d82a-4ded-4006-8942-3f3db29545a31584617209...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>fbaeaf7c-8021-431f-9e40-e90d3f4cac4a1584617208...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>ba8e93d3-db25-46d3-b753-ca3da6db74441566387825...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Item                                                img\n",
              "0  Tshirts  79610782-69fe-4653-8c43-9024366a9a581556619128...\n",
              "1  Tshirts  6b60f13a-8d7d-4e0d-abac-ab83b58882bc1556619128...\n",
              "2  Tshirts  3711d82a-4ded-4006-8942-3f3db29545a31584617209...\n",
              "3  Tshirts  fbaeaf7c-8021-431f-9e40-e90d3f4cac4a1584617208...\n",
              "4  Tshirts  ba8e93d3-db25-46d3-b753-ca3da6db74441566387825..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3BAXL1KHDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "ds=pd.read_csv('ds.csv', error_bad_lines=False,sep='\\t')\n",
        "dt=pd.read_csv('dt.csv', error_bad_lines=False,sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQj8Tz-xKHEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds.Item = ds.Item.astype(str)\n",
        "dt.Item = dt.Item.astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMnYkBB2k4L",
        "colab_type": "code",
        "outputId": "e14aea4d-eef2-4ba5-a4ed-4a114204c322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "niq=np.unique(ds[['Item']].values)\n",
        "niq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Jackets', 'Kurtas', 'Shirts'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "620ZEE4chJYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxFtCP8_20qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in dt.index:\n",
        "    \n",
        "\n",
        "    if dt.loc[i,'Item']== 'Blazers':\n",
        "        dt.loc[i,'Item']= 'Jackets'\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avyI8SLaKHEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dataGen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  shear_range = 0.2,\n",
        "                                  zoom_range = 0.2,\n",
        "                                  horizontal_flip = True,\n",
        "                                  validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_oIq7DeKHFb",
        "colab_type": "code",
        "outputId": "d9eb34ad-ff6c-47f3-b136-d5be141d2ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "train_generator = train_dataGen.flow_from_dataframe(\n",
        "                                        dataframe = ds,\n",
        "                                        directory=r\"/content/Img\",x_col=\"img\",\n",
        "                                        y_col=\"Item\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        target_size=(128,128),\n",
        "                                        batch_size=32,\n",
        "                                        subset='training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14761 validated image filenames belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"img\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vItyXMbKHGK",
        "colab_type": "code",
        "outputId": "91b98437-9450-4328-8a6b-200b8e3ef512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_generator# histgram"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.image.dataframe_iterator.DataFrameIterator at 0x7ff3e5be2c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWvl0UbPKHGh",
        "colab_type": "code",
        "outputId": "4606d57e-25c8-48cb-d4bd-21d8768176b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "val_generator = train_dataGen.flow_from_dataframe(\n",
        "                                        dataframe = ds,\n",
        "                                        directory=r\"/content/Img\",x_col=\"img\",\n",
        "                                        y_col=\"Item\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        target_size=(128,128),\n",
        "                                        batch_size=32,\n",
        "                                        subset='validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3690 validated image filenames belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 1 invalid image filename(s) in x_col=\"img\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjZTlb0jKHG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds['img'] = ds['img'].str.replace('.jpg.jpg', '.jpg')\n",
        "dt['img'] = dt['img'].str.replace('.jpg.jpg', '.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "93rJ6OEbKHHR",
        "colab_type": "code",
        "outputId": "eba1a374-0651-4a6a-d105-eed9c9f4643f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "classifier = Sequential()\n",
        "classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape = (128,128,3)))\n",
        "classifier.add(BatchNormalization())\n",
        "\n",
        "classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(512, activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Dense(128, activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.5))\n",
        "\n",
        "classifier.add(Dense(3, activation='softmax'))\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy','accuracy'])\n",
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 126, 126, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 124, 124, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 124, 124, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 60, 60, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 60, 60, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 60, 60, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 58, 58, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 58, 58, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 29, 29, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 29, 29, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 107648)            0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               55116288  \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 55,296,739\n",
            "Trainable params: 55,294,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAH_h4CPKHHl",
        "colab_type": "code",
        "outputId": "767d1237-412e-4f43-fbe3-cfd1a3c4c9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(r'/content/fashion_item1_img4.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FixdQlg5KHH-",
        "colab_type": "code",
        "outputId": "a8fd464a-7e74-4d4b-faf1-f602c0fe1bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "hist2=classifier.fit(train_generator, \n",
        "    steps_per_epoch = 60,\n",
        "    validation_data = val_generator, \n",
        "    validation_steps = 60,\n",
        "    epochs = 100,\n",
        "    verbose=0, \n",
        "    #class_weight=None,\n",
        "    callbacks=[ mcp_save,earlyStopping,reduce_lr_loss], use_multiproceessing=True,  workers=-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H63hWoqCSS0h",
        "colab_type": "code",
        "outputId": "f7ec43d8-403a-4e47-e483-a970533fc68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(hist.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [1.9327590465545654, 1.7041947841644287, 1.4966083765029907, 1.4503370523452759, 1.371610164642334, 1.3185482025146484, 1.2680429220199585, 1.1968997716903687, 1.185969591140747, 1.175963282585144, 1.1285393238067627, 1.0980392694473267, 1.1072593927383423, 1.0789705514907837, 1.0721652507781982, 1.0440086126327515, 1.119264841079712, 1.0633827447891235, 1.0725042819976807, 1.0107094049453735, 0.9970390796661377, 0.997394323348999, 0.9891633987426758, 0.9810337424278259, 0.9754011034965515, 0.9849826693534851, 0.9819076061248779, 0.9900075197219849, 0.9643388986587524, 0.9817947745323181], 'categorical_accuracy': [0.30781251192092896, 0.3192708194255829, 0.3864583373069763, 0.3932291567325592, 0.40937501192092896, 0.43802082538604736, 0.45781248807907104, 0.4913020431995392, 0.4828124940395355, 0.4906249940395355, 0.5161458253860474, 0.5338541865348816, 0.5364583134651184, 0.5406249761581421, 0.559374988079071, 0.5651028156280518, 0.5234375, 0.5505208373069763, 0.5453125238418579, 0.5651041865348816, 0.5958333611488342, 0.5916666388511658, 0.5880208611488342, 0.6109646558761597, 0.5979166626930237, 0.5869791507720947, 0.598437488079071, 0.5880208611488342, 0.6177083253860474, 0.5921875238418579], 'accuracy': [0.30781251192092896, 0.3192708194255829, 0.3864583373069763, 0.3932291567325592, 0.40937501192092896, 0.43802082538604736, 0.45781248807907104, 0.4913020431995392, 0.4828124940395355, 0.4906249940395355, 0.5161458253860474, 0.5338541865348816, 0.5364583134651184, 0.5406249761581421, 0.559374988079071, 0.5651028156280518, 0.5234375, 0.5505208373069763, 0.5453125238418579, 0.5651041865348816, 0.5958333611488342, 0.5916666388511658, 0.5880208611488342, 0.6109646558761597, 0.5979166626930237, 0.5869791507720947, 0.598437488079071, 0.5880208611488342, 0.6177083253860474, 0.5921875238418579], 'val_loss': [1.2754405736923218, 1.6094456911087036, 2.021019458770752, 0.9362415075302124, 1.099584698677063, 0.8919198513031006, 1.00190269947052, 0.9504756927490234, 0.7270950078964233, 0.6215649843215942, 1.3474403619766235, 1.0345033407211304, 1.6420104503631592, 1.077043890953064, 1.2839534282684326, 0.9826404452323914, 1.5821404457092285, 1.2987878322601318, 1.1356971263885498, 1.1153448820114136, 1.0378297567367554, 1.021952509880066, 1.003659725189209, 0.985798716545105, 0.990796685218811, 0.9746785759925842, 0.9710407257080078, 0.9600867629051208, 0.9705215692520142, 0.9557510614395142], 'val_categorical_accuracy': [0.10989583283662796, 0.0010537407360970974, 0.0026041667442768812, 0.7007375955581665, 0.6015625, 0.7065331935882568, 0.6963541507720947, 0.7660695314407349, 0.8708333373069763, 0.9025290012359619, 0.03281249850988388, 0.5379346609115601, 0.2005208283662796, 0.508956789970398, 0.3072916567325592, 0.6422550082206726, 0.15052083134651184, 0.30136987566947937, 0.4281249940395355, 0.453108549118042, 0.5083333253860474, 0.5131717324256897, 0.5385416746139526, 0.5400421619415283, 0.5333333611488342, 0.5569019913673401, 0.5395833253860474, 0.5616438388824463, 0.5537407994270325, 0.5703125], 'val_accuracy': [0.10989583283662796, 0.0010537407360970974, 0.0026041667442768812, 0.7007375955581665, 0.6015625, 0.7065331935882568, 0.6963541507720947, 0.7660695314407349, 0.8708333373069763, 0.9025290012359619, 0.03281249850988388, 0.5379346609115601, 0.2005208283662796, 0.508956789970398, 0.3072916567325592, 0.6422550082206726, 0.15052083134651184, 0.30136987566947937, 0.4281249940395355, 0.453108549118042, 0.5083333253860474, 0.5131717324256897, 0.5385416746139526, 0.5400421619415283, 0.5333333611488342, 0.5569019913673401, 0.5395833253860474, 0.5616438388824463, 0.5537407994270325, 0.5703125], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvgeagD50ugs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVIvK_vtgED",
        "colab_type": "code",
        "outputId": "397fdbba-5b1c-4dd8-a5be-66898d6f0f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(hist2.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [0.7422354817390442, 0.749257504940033, 0.7446734309196472, 0.7419682145118713, 0.7275286912918091, 0.7241052985191345, 0.7246903777122498, 0.7820433974266052, 0.7177726030349731, 0.7331846356391907, 0.7126319408416748, 0.7252413034439087, 0.7396003603935242, 0.7352413535118103, 0.7430150508880615, 0.7168749570846558, 0.7118908762931824, 0.7339773178100586, 0.7244578003883362, 0.7378423810005188, 0.69426429271698, 0.719736635684967, 0.7445850968360901, 0.7427771687507629, 0.7130680680274963, 0.7050154805183411, 0.7354642748832703, 0.750643789768219, 0.7054051160812378, 0.7355037331581116, 0.7256947755813599, 0.7003949880599976], 'categorical_accuracy': [0.6921454668045044, 0.7005208134651184, 0.6916666626930237, 0.7088541388511658, 0.7041666507720947, 0.7166666388511658, 0.7098958492279053, 0.6758038997650146, 0.7145833373069763, 0.7010416388511658, 0.723437488079071, 0.706250011920929, 0.7052083611488342, 0.7098958492279053, 0.698437511920929, 0.709541380405426, 0.7208333611488342, 0.7161458134651184, 0.7119791507720947, 0.7041666507720947, 0.7286458611488342, 0.7119791507720947, 0.7098958492279053, 0.7121770977973938, 0.7104166746139526, 0.7135416865348816, 0.7114583253860474, 0.6973958611488342, 0.7192708253860474, 0.706250011920929, 0.7047970294952393, 0.723437488079071], 'accuracy': [0.6921454668045044, 0.7005208134651184, 0.6916666626930237, 0.7088541388511658, 0.7041666507720947, 0.7166666388511658, 0.7098958492279053, 0.6758038997650146, 0.7145833373069763, 0.7010416388511658, 0.723437488079071, 0.706250011920929, 0.7052083611488342, 0.7098958492279053, 0.698437511920929, 0.709541380405426, 0.7208333611488342, 0.7161458134651184, 0.7119791507720947, 0.7041666507720947, 0.7286458611488342, 0.7119791507720947, 0.7098958492279053, 0.7121770977973938, 0.7104166746139526, 0.7135416865348816, 0.7114583253860474, 0.6973958611488342, 0.7192708253860474, 0.706250011920929, 0.7047970294952393, 0.723437488079071], 'val_loss': [0.29448044300079346, 0.29131415486335754, 0.30111315846443176, 0.2966848313808441, 0.3008098900318146, 0.2975459396839142, 0.29956284165382385, 0.28437089920043945, 0.2913898527622223, 0.2950070798397064, 0.2902606427669525, 0.2837129533290863, 0.30670228600502014, 0.2894120514392853, 0.2943834960460663, 0.30261796712875366, 0.31315967440605164, 0.3044849932193756, 0.2910870611667633, 0.3019447922706604, 0.2975117862224579, 0.3087784945964813, 0.30306071043014526, 0.3063022196292877, 0.2906923294067383, 0.30359116196632385, 0.3041781485080719, 0.2956390976905823, 0.3030327260494232, 0.29959747195243835, 0.3082706034183502, 0.3042140603065491], 'val_categorical_accuracy': [0.973437488079071, 0.9778714179992676, 0.96875, 0.9726027250289917, 0.9683877825737, 0.9708333611488342, 0.9673340320587158, 0.9760416746139526, 0.9726027250289917, 0.9703124761581421, 0.9668071866035461, 0.9765625, 0.9599578380584717, 0.9755208492279053, 0.9683877825737, 0.9744791388511658, 0.9641728401184082, 0.9619791507720947, 0.9699683785438538, 0.9645833373069763, 0.9678608775138855, 0.9666666388511658, 0.9610115885734558, 0.9635416865348816, 0.9768177270889282, 0.9604166746139526, 0.9599578380584717, 0.9661458134651184, 0.9678608775138855, 0.9677083492279053, 0.959430992603302, 0.9624999761581421], 'val_accuracy': [0.973437488079071, 0.9778714179992676, 0.96875, 0.9726027250289917, 0.9683877825737, 0.9708333611488342, 0.9673340320587158, 0.9760416746139526, 0.9726027250289917, 0.9703124761581421, 0.9668071866035461, 0.9765625, 0.9599578380584717, 0.9755208492279053, 0.9683877825737, 0.9744791388511658, 0.9641728401184082, 0.9619791507720947, 0.9699683785438538, 0.9645833373069763, 0.9678608775138855, 0.9666666388511658, 0.9610115885734558, 0.9635416865348816, 0.9768177270889282, 0.9604166746139526, 0.9599578380584717, 0.9661458134651184, 0.9678608775138855, 0.9677083492279053, 0.959430992603302, 0.9624999761581421], 'lr': [1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-07, 1.0000001e-07, 1.0000001e-07, 1.0000001e-07, 1.0000001e-07, 1.0000001e-07]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhCnJFr9cKh4",
        "colab_type": "code",
        "outputId": "d8ae4bb9-3c33-42f8-8ed7-a72b05c9565e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(hist1.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [0.9069541096687317, 0.8142120838165283, 0.7694836258888245, 0.6967082619667053, 0.6464230418205261, 0.6351019144058228, 0.6016111969947815, 0.5807608366012573, 0.5340478420257568, 0.5619154572486877, 0.5645735263824463, 0.5481702089309692, 0.5305150151252747, 0.5393761992454529, 0.5458049178123474, 0.5390971302986145, 0.5290108323097229, 0.5223191976547241, 0.5285526514053345, 0.526262104511261, 0.5175787806510925], 'categorical_accuracy': [0.6098958253860474, 0.6239583492279053, 0.621874988079071, 0.6328125, 0.6557291746139526, 0.6703125238418579, 0.6796875, 0.7084870934486389, 0.7270833253860474, 0.7041666507720947, 0.707812488079071, 0.7161458134651184, 0.7281249761581421, 0.7192708253860474, 0.7192708253860474, 0.7316815853118896, 0.7369791865348816, 0.7354166507720947, 0.7265625, 0.7307291626930237, 0.7432291507720947], 'accuracy': [0.6098958253860474, 0.6239583492279053, 0.621874988079071, 0.6328125, 0.6557291746139526, 0.6703125238418579, 0.6796875, 0.7084870934486389, 0.7270833253860474, 0.7041666507720947, 0.707812488079071, 0.7161458134651184, 0.7281249761581421, 0.7192708253860474, 0.7192708253860474, 0.7316815853118896, 0.7369791865348816, 0.7354166507720947, 0.7265625, 0.7307291626930237, 0.7432291507720947], 'val_loss': [0.03830300271511078, 0.34014979004859924, 0.42301902174949646, 0.3857639729976654, 0.46997350454330444, 0.2880266308784485, 0.3906191289424896, 0.3756057620048523, 0.4054310619831085, 0.5076863765716553, 0.5221892595291138, 0.5352067947387695, 0.5909217000007629, 0.5996749401092529, 0.5987280607223511, 0.623425304889679, 0.5957803726196289, 0.6271016597747803, 0.618462860584259, 0.6101066470146179, 0.6363667249679565], 'val_categorical_accuracy': [1.0, 0.9957850575447083, 0.9583333134651184, 1.0, 0.995312511920929, 1.0, 0.9276041388511658, 0.944151759147644, 0.895312488079071, 0.7876712083816528, 0.7682291865348816, 0.7397260069847107, 0.7005208134651184, 0.6954689025878906, 0.6807291507720947, 0.6649104356765747, 0.6875, 0.6659641861915588, 0.6776041388511658, 0.6664910316467285, 0.659375011920929], 'val_accuracy': [1.0, 0.9957850575447083, 0.9583333134651184, 1.0, 0.995312511920929, 1.0, 0.9276041388511658, 0.944151759147644, 0.895312488079071, 0.7876712083816528, 0.7682291865348816, 0.7397260069847107, 0.7005208134651184, 0.6954689025878906, 0.6807291507720947, 0.6649104356765747, 0.6875, 0.6659641861915588, 0.6776041388511658, 0.6664910316467285, 0.659375011920929], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAt7hA4KHIb",
        "colab_type": "code",
        "outputId": "e077788d-b36b-4f30-9cf1-b303b1426004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "classes = train_generator.class_indices\n",
        "print(classes)\n",
        "inverted_classes = dict(map(reversed, classes.items()))\n",
        "print(inverted_classes)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Jackets': 0, 'Kurtas': 1, 'Shirts': 2}\n",
            "{0: 'Jackets', 1: 'Kurtas', 2: 'Shirts'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RtTR2mdvneH",
        "colab_type": "code",
        "outputId": "7a05f9fe-719f-4d91-8f89-fce8515ccee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "dr=ds.sample(n=5000)\n",
        "dr.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item</th>\n",
              "      <th>img</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10916</th>\n",
              "      <td>Jackets</td>\n",
              "      <td>2ea89558-f370-440c-8a41-054c75a2469b1582873020...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1770</th>\n",
              "      <td>Shirts</td>\n",
              "      <td>58f23454-3195-4340-a733-7ff4431a9ab21581310359...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18112</th>\n",
              "      <td>Jackets</td>\n",
              "      <td>6cf52baa-cb3a-4f70-beba-8c2418628dea1568885085...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>Shirts</td>\n",
              "      <td>72822e1b-c2a0-4bd7-b58b-d00c5aee5d5f1582148632...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5560</th>\n",
              "      <td>Shirts</td>\n",
              "      <td>45d2efe9-5162-41cc-8941-6e74bec7507e1567672432...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Item                                                img\n",
              "10916  Jackets  2ea89558-f370-440c-8a41-054c75a2469b1582873020...\n",
              "1770    Shirts  58f23454-3195-4340-a733-7ff4431a9ab21581310359...\n",
              "18112  Jackets  6cf52baa-cb3a-4f70-beba-8c2418628dea1568885085...\n",
              "1033    Shirts  72822e1b-c2a0-4bd7-b58b-d00c5aee5d5f1582148632...\n",
              "5560    Shirts  45d2efe9-5162-41cc-8941-6e74bec7507e1567672432..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spzHfIyvKHJQ",
        "colab_type": "code",
        "outputId": "ace29996-72e0-45f7-bd01-09e9112c0689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier.evaluate_generator(generator=val_generator,\n",
        "steps=60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31379255652427673, 0.9557428956031799, 0.9557428956031799]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1Dr396KHJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "Y_pred = []\n",
        "\n",
        "for i in range(len(dr)):\n",
        "    img = image.load_img(path= r'/content/Img/{}'.format(dr.img[i]),target_size=(128,128,3))\n",
        "    img = image.img_to_array(img)\n",
        "    test_img = img.reshape((1,128,128,3))\n",
        "    img_class = classifier.predict_classes(test_img)\n",
        "    prediction = img_class[0]\n",
        "    Y_pred.append(prediction)    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZhkl4rKKHKL",
        "colab_type": "code",
        "outputId": "a24fb702-f458-4bb3-d88f-bb8280108ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(Y_pred)\n",
        "\n",
        "prediction_classes = [ inverted_classes.get(item,item) for item in Y_pred ]\n",
        "print(prediction_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "['Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Jackets', 'Kurtas', 'Jackets']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HetS_6W_KHKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1=dr.assign(prediction_classes=prediction_classes)\n",
        "t1['result'] = np.where(t1['Item'] == t1['prediction_classes'], 'Correct', 'Incorrect')\n",
        "t1.groupby('result').count()\n",
        "t1.to_csv(r'/content/drive5.csv', encoding='utf-8', mode='a', header=True, index=None,sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTy9JYXRTRRX",
        "colab_type": "code",
        "outputId": "6c8ed9d1-2ec4-4abc-8838-02f650833d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J75PBbtAKHKs",
        "colab_type": "code",
        "outputId": "b55f5683-5d2e-4486-83e6-3d4f7d81e4e9",
        "colab": {}
      },
      "source": [
        "dt.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item</th>\n",
              "      <th>img</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>6b60f13a-8d7d-4e0d-abac-ab83b58882bc1556619128...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>3711d82a-4ded-4006-8942-3f3db29545a31584617209...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>fbaeaf7c-8021-431f-9e40-e90d3f4cac4a1584617208...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>ba8e93d3-db25-46d3-b753-ca3da6db74441566387825...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tshirts</td>\n",
              "      <td>740ebd32-ae33-421b-b41e-85326c24faf21566387825...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Item                                                img\n",
              "0  Tshirts  6b60f13a-8d7d-4e0d-abac-ab83b58882bc1556619128...\n",
              "1  Tshirts  3711d82a-4ded-4006-8942-3f3db29545a31584617209...\n",
              "2  Tshirts  fbaeaf7c-8021-431f-9e40-e90d3f4cac4a1584617208...\n",
              "3  Tshirts  ba8e93d3-db25-46d3-b753-ca3da6db74441566387825...\n",
              "4  Tshirts  740ebd32-ae33-421b-b41e-85326c24faf21566387825..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPpjxJIKHLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dt=dt.drop(0)\n",
        "dr = dr.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Civy5OogKHLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt=dt[dt.img !='img']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuL28L5jKHLb",
        "colab_type": "code",
        "outputId": "a8d1bb69-8cd6-42dd-e67d-19b68763a257",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load training and test data into dataframes\n",
        "\n",
        "\n",
        "# X forms the training images, and y forms the training labels\n",
        "X = np.array(ds.loc[:, 'img'])\n",
        "y = to_categorical(np.array(ds.loc[:, 'Item']))\n",
        "\n",
        "# Here I split original training data to sub-training (80%) and validation data (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\n",
        "\n",
        "# X_test forms the test images, and y_test forms the test labels\n",
        "X_test = np.array(dt.loc[:, 'img'])\n",
        "y_test = to_categorical(np.array(dt.loc[:,'Item']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'Tshirts'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-60-8633c94e5177>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# X forms the training images, and y forms the training labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'img'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Item'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Here I split original training data to sub-training (80%) and validation data (20%)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Tshirts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_kYZfgXKHL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((150,150))) for im in X])\n",
        "X_test = np.asarray([img_to_array(array_to_img(im, scale=False).resize((150,150))) for im in X_test])\n",
        "\n",
        "# Display the new shape\n",
        "X.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvBN5N6CKHMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each image's dimension is 28 x 28\n",
        "img_rows, img_cols = 32, 32\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Prepare the training images\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255\n",
        "\n",
        "# Prepare the test images\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.astype('float32')\n",
        "X_test /= 255\n",
        "\n",
        "# Prepare the validation images\n",
        "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
        "X_val = X_val.astype('float32')\n",
        "X_val /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqjUz9kXKHMq",
        "colab_type": "code",
        "outputId": "fa9a96ed-ad9e-467c-c439-5d277194f61e",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Create the base model of VGG19\n",
        "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape = (150, 150, 3), classes = 7)\n",
        "\n",
        "\n",
        "img_path = r'C:\\Users\\Manas\\Desktop\\ML\\Capstoneproject\\img'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "# Preprocessing the input \n",
        "X_train = preprocess_input(train_generator)\n",
        "X_val = preprocess_input(val_generator)\n",
        "#X_test = preprocess_input(X_test)\n",
        "\n",
        "# Extracting features\n",
        "train_features = vgg19.predict(np.array(train_generator), batch_size=256, verbose=1)\n",
        "#test_features = vgg19.predict(np.array(X_test), batch_size=256, verbose=1)\n",
        "val_features = vgg19.predict(np.array(val_generator), batch_size=256, verbose=1)\n",
        "\n",
        "# Flatten extracted features\n",
        "train_features = np.reshape(train_features, (48000, 4*4*512))\n",
        "#test_features = np.reshape(test_features, (10000, 4*4*512))\n",
        "val_features = np.reshape(val_features, (12000, 4*4*512))\n",
        "\n",
        "# Add Dense and Dropout layers on top of VGG19 pre-trained\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(7, activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 247s 3us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'>=' not supported between instances of 'tuple' and 'int'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-58-17c4f24dacfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Preprocessing the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#X_test = preprocess_input(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg19.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[1;34m(x, data_format)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m   return imagenet_utils.preprocess_input(\n\u001b[1;32m--> 237\u001b[1;33m       x, data_format=data_format, mode='caffe')\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[1;34m(x, data_format, mode)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     return _preprocess_symbolic_input(\n\u001b[1;32m--> 110\u001b[1;33m         x, data_format=data_format, mode=mode)\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36m_preprocess_symbolic_input\u001b[1;34m(x, data_format, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m       \u001b[1;31m# 'RGB'->'BGR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m103.939\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.779\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m123.68\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             raise ValueError('Asked to retrieve element {idx}, '\n\u001b[0;32m     55\u001b[0m                              \u001b[1;34m'but the Sequence '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'tuple' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe-ft3QbKHNQ",
        "colab_type": "code",
        "outputId": "9dfa20a6-576d-4274-9338-97994d1fbbc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(r'/content/fashion_item_img.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRFVxgbNKHNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_generator, \n",
        "    steps_per_epoch = 60,\n",
        "    validation_data = val_generator, \n",
        "    validation_steps = 60,\n",
        "    epochs = 150,\n",
        "    verbose=0, \n",
        "    #class_weight=None,\n",
        "    callbacks=[earlyStopping, mcp_save])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP95wI8vKHNx",
        "colab_type": "code",
        "outputId": "c7afd4a7-b94b-405e-b66d-7ef6fc6ab6fc",
        "colab": {}
      },
      "source": [
        "vggmodel = VGG19(weights='imagenet', include_top=True)\n",
        "vggmodel.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574717952/574710816 [==============================] - 1546s 3us/step\n",
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 143,667,240\n",
            "Trainable params: 143,667,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nUH9KXKHN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layers in (vggmodel.layers)[:19]:\n",
        "    print(layers)\n",
        "    layers.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H86I-VJYKHOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X= vggmodel.layers[-2].output\n",
        "predictions = Dense(2, activation=\"softmax\")(X)\n",
        "model_final = Model(input = vggmodel.input, output = predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3-vhkgyKHOa",
        "colab_type": "code",
        "outputId": "f9ca8a39-1f63-4253-c318-30bced2e6c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "base_model = VGG19(weights='imagenet', include_top=False)\n",
        "\n",
        "x=base_model.output                                                          \n",
        "x=GlobalAveragePooling2D()(x)                                                \n",
        "x=Dense(1024,activation='relu')(x)                                           \n",
        "x=Dense(1024,activation='relu')(x)                                           \n",
        "x=Dense(512,activation='relu')(x)        \n",
        "\n",
        "\n",
        "\n",
        "preds=Dense(7,activation='softmax')(x)                                      \n",
        "model1=Model(inputs=base_model.input,outputs=preds)\n",
        "model1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy','accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_4 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 22,127,687\n",
            "Trainable params: 22,127,687\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eT0xlykKHOl",
        "colab_type": "code",
        "outputId": "f2197d5c-1817-4d4c-ec7b-7159c7bed11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "hist=model1.fit(train_generator, \n",
        "    steps_per_epoch = 60,\n",
        "    validation_data = val_generator, \n",
        "    validation_steps = 60,\n",
        "    epochs = 100,\n",
        "    verbose=0, \n",
        "    #class_weight=None,\n",
        "    callbacks=[earlyStopping, mcp_save,reduce_lr_loss])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP4Leu_GKHOx",
        "colab_type": "code",
        "outputId": "a624593a-0d08-4979-9329-d9718c8156c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(hist.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [1.6248918771743774, 1.6294301748275757, 1.5428575277328491, 1.452626347541809, 1.4019725322723389, 1.4285389184951782, 1.3980637788772583, 1.359805941581726, 1.285906195640564, 1.279422402381897, 1.2442134618759155, 1.2074618339538574], 'categorical_accuracy': [0.38854166865348816, 0.35885417461395264, 0.3947916626930237, 0.4151041805744171, 0.446494460105896, 0.43802082538604736, 0.4312500059604645, 0.4557291567325592, 0.4739583432674408, 0.49270832538604736, 0.5005208253860474, 0.5276041626930237], 'accuracy': [0.38854166865348816, 0.35885417461395264, 0.3947916626930237, 0.4151041805744171, 0.446494460105896, 0.43802082538604736, 0.4312500059604645, 0.4557291567325592, 0.4739583432674408, 0.49270832538604736, 0.5005208253860474, 0.5276041626930237], 'val_loss': [2.1976122856140137, 2.190953493118286, 2.508315324783325, 2.9769177436828613, 2.917428493499756, 3.0043797492980957, 2.7322959899902344, 2.646991491317749, 2.710944652557373, 2.600999355316162, 2.8704259395599365, 2.9172890186309814], 'val_categorical_accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}